package expr

import (
	"errors"
	"fmt"
	"io"

	"github.com/raintank/metrictank/api/models"
	"github.com/raintank/metrictank/consolidation"
)

type Req struct {
	Query string
	From  uint32 // from for this particular pattern
	To    uint32 // to for this particular pattern
}

type Plan struct {
	Reqs          []Req
	expr          *expr
	MaxDataPoints uint32
	From          uint32                     // global request scoped from
	To            uint32                     // global request scoped to
	ConsolidateBy consolidation.Consolidator // consolidateBy that all requests should use
	input         map[Req][]models.Series    // input data to work with. set via Run()
	stable        bool                       // restrict to stable functions only
	// new data generated by processing funcs. useful for two reasons:
	// 1) reuse partial calculations e.g. queries like target=movingAvg(sum(foo), 10)&target=sum(foo)A (TODO)
	// 2) central place to return data back to pool when we're done.
	generated map[Req][]models.Series

	// when the plan is run, this is the handler we should execute.
	entryPoint execHandler
}

func (p Plan) Dump(w io.Writer) {
	fmt.Fprintf(w, "Plan:\n")
	fmt.Fprintf(w, "* Expr:\n")
	fmt.Fprintln(w, p.expr.Print(2))
	fmt.Fprintf(w, "* Reqs:\n")
	for _, r := range p.Reqs {
		fmt.Fprintln(w, "   ", r)
	}
	fmt.Fprintf(w, "MaxDataPoints: %d\n", p.MaxDataPoints)
	fmt.Fprintf(w, "From: %d\n", p.From)
	fmt.Fprintf(w, "To: %d\n", p.To)
	fmt.Fprintf(w, "ConsolidateBy: %s\n", p.ConsolidateBy.String())
}

// Plan validates the expressions and comes up with the initial (potentially non-optimal) execution plan
// which is just a list of requests and the expressions.
// traverse tree and as we go down:
// * make sure function exists
// * tentative validation pre function call (number of args and type of args, to the extent it can be done in advance),
// * let function validate input arguments further (to the extend it can be done in advance)
// * allow functions to extend the notion of which data is required
// * future version: allow functions to mark safe to pre-aggregate using consolidateBy or not
func NewPlan(expr *expr, from, to, mdp uint32, stable bool, reqs []Req) (*Plan, error) {
	var err error
	plan := &Plan{
		Reqs:          reqs,
		expr:          expr,
		MaxDataPoints: mdp,
		From:          from,
		To:            to,
		ConsolidateBy: consolidation.Avg,
		stable:        stable,
	}

	if expr.etype != etFunc && expr.etype != etName {
		return nil, errors.New("request must be a function call or metric pattern")
	}

	handler, err := plan.GetHandler(expr)
	if err != nil {
		return nil, err
	}
	plan.entryPoint = handler

	return plan, nil
}

func (p *Plan) GetHandler(e *expr) (execHandler, error) {
	if e.etype == etName {
		return p.fetchSeries(e.str)
	}

	if e.etype != etFunc {
		return nil, ErrBadArgumentStr{"func or name", string(e.etype)}
	}

	// here e.type is guaranteed to be etFunc
	fdef, ok := funcs[e.str]
	if !ok {
		return nil, ErrUnknownFunction(e.str)
	}
	if p.stable && !fdef.stable {
		return nil, ErrUnknownFunction(e.str)
	}
	gf := fdef.constr()
	return gf.Plan(e.args, e.namedArgs, p)
}

// Run invokes all processing as specified in the plan (expressions, from/to) with the input as input
func (p *Plan) Run(input map[Req][]models.Series) ([]models.Series, error) {
	p.input = input
	p.generated = make(map[Req][]models.Series)

	return p.entryPoint(p.generated)
}

// Clean returns all buffers (all input data + generated series along the way)
// back to the pool.
func (p *Plan) Clean() {
	for _, series := range p.input {
		for _, serie := range series {
			pointSlicePool.Put(serie.Datapoints[:0])
		}
	}
	for _, series := range p.generated {
		for _, serie := range series {
			pointSlicePool.Put(serie.Datapoints[:0])
		}
	}
}

func (p *Plan) fetchData(cache map[Req][]models.Series, req Req) ([]models.Series, error) {
	return p.input[req], nil
}

func (p *Plan) fetchSeries(query string) (execHandler, error) {
	req := Req{
		query,
		p.From,
		p.To,
	}
	p.Reqs = append(p.Reqs, req)

	return func(cache map[Req][]models.Series) ([]models.Series, error) {
		return p.fetchData(cache, req)
	}, nil
}
